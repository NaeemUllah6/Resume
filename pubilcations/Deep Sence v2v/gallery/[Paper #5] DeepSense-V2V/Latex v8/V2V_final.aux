\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{harding2014vehicle}
\citation{zeadally2020tutorial}
\citation{rappaport2019wireless}
\citation{alkhateeb2023real,viswanathan2020communications}
\citation{charan2022vision,charan2021vision,morais2022position,charan2022multi,jiang2022lidar,demirhan2022radar,charan2022drone,wu2023proactively,charan2022blockage}
\citation{demirhan2023integrated}
\citation{caesar2020nuscenes,llc2019waymo,pham20203d}
\citation{Brostow_CamVid_2008}
\citation{Geiger_KITTI}
\citation{cordts2016cityscapes}
\citation{yu2018bdd100k}
\citation{huang2019apolloscape}
\citation{ma2019trafficpredict}
\citation{patil2019h3d}
\citation{caesar2020nuscenes}
\citation{chang2019argoverse}
\citation{houston2020thousand}
\citation{llc2019waymo}
\citation{pham20203d}
\citation{wang2021rethinking}
\citation{yu2022dair}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:Intro}{{I}{1}{Introduction}{section.1}{}}
\citation{Brostow_CamVid_2008,Geiger_KITTI,cordts2016cityscapes,yu2018bdd100k,huang2019apolloscape,ma2019trafficpredict,patil2019h3d,chang2019argoverse,caesar2020nuscenes,llc2019waymo,pham20203d,wang2021rethinking,yu2022dair,choi2018kaist}
\citation{Geiger_KITTI}
\citation{patil2019h3d}
\citation{choi2018kaist}
\citation{caesar2020nuscenes}
\citation{llc2019waymo}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparative summary of key characteristics of state-of-the-art vehicular datasets.}}{2}{table.1}\protected@file@percent }
\newlabel{tab:literature_review}{{I}{2}{Comparative summary of key characteristics of state-of-the-art vehicular datasets}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Literature Review}{2}{section.2}\protected@file@percent }
\newlabel{sec:Literature}{{II}{2}{Literature Review}{section.2}{}}
\citation{collision_warning_v2v}
\citation{collision_avoidance_60_percent}
\citation{p2p_video_streaming}
\citation{DeepSense}
\@writefile{toc}{\contentsline {section}{\numberline {III}DeepSense V2V Testbed and Scenario Creation}{3}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces DeepSense V2V testbed setup overview. For more information on the testbed visit: \href  {https://www.deepsense6g.net/data-collection/}{Testbed6}}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:V2V_testbed}{{1}{3}{DeepSense V2V testbed setup overview. For more information on the testbed visit: \href {https://www.deepsense6g.net/data-collection/}{Testbed6}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Data Collection}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:Collection}{{\mbox  {III-A}}{3}{Data Collection}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Overview of general DeepSense structure that was used in the creation of the V2V Scenarios.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:deepsense_overview}{{2}{4}{Overview of general DeepSense structure that was used in the creation of the V2V Scenarios}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces CAD design with dimensions of V2V box placement on car.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:sensor_testbed_schematic}{{3}{4}{CAD design with dimensions of V2V box placement on car}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Data Processing}{4}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:Processing}{{\mbox  {III-B}}{4}{Data Processing}{subsection.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Description of the sensors used in the DeepSense-V2V testbed.}}{5}{table.2}\protected@file@percent }
\newlabel{tab:sensors}{{II}{5}{Description of the sensors used in the DeepSense-V2V testbed}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Frame of sample 4038 from video of Scenario 36. The current template shows four 90º camera views rendered around the car, the lidar pointcloud colored based on distance, four radar range-velocity plots, a GPS with the locations of the vehicles scattered on top of the satellite image of the location, and four 64-beam power vectors with the normalized received power in each beam. The video rendered for Scenario 36 data can be watched on \href  {https://www.youtube.com/watch?v=9RyZnZI7kv0&ab_channel=DeepSense6G}{YouTube}}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:data_visualization}{{4}{6}{Frame of sample 4038 from video of Scenario 36. The current template shows four 90º camera views rendered around the car, the lidar pointcloud colored based on distance, four radar range-velocity plots, a GPS with the locations of the vehicles scattered on top of the satellite image of the location, and four 64-beam power vectors with the normalized received power in each beam. The video rendered for Scenario 36 data can be watched on \href {https://www.youtube.com/watch?v=9RyZnZI7kv0&ab_channel=DeepSense6G}{YouTube}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Data Visualization}{6}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:Visualization}{{\mbox  {III-C}}{6}{Data Visualization}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Dataset Statistics}{6}{section.4}\protected@file@percent }
\newlabel{sec:Statistics}{{IV}{6}{Dataset Statistics}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Vehicle Locations and Velocities}{6}{subsection.4.1}\protected@file@percent }
\newlabel{subsec:loc_and_vel}{{\mbox  {IV-A}}{6}{Vehicle Locations and Velocities}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Satellite images with the locations of each scenario. Also included are several macro statistics of the data collection, providing contextual and objective information derived mainly from the GPS sensors.}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:locs}{{5}{7}{Satellite images with the locations of each scenario. Also included are several macro statistics of the data collection, providing contextual and objective information derived mainly from the GPS sensors}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Cumulative distribution of vehicle speed with indication of speed limits (bold vertical lines) and the type of road in which the limits are enforced. Urban and rural scenarios are marked and can be recognized by the average speed and by a shorter tail in the speed distribution.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:speeds}{{6}{7}{Cumulative distribution of vehicle speed with indication of speed limits (bold vertical lines) and the type of road in which the limits are enforced. Urban and rural scenarios are marked and can be recognized by the average speed and by a shorter tail in the speed distribution}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Inter-vehicle Distance and Received Power}{7}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:distance_vs_receivepower}{{\mbox  {IV-B}}{7}{Inter-vehicle Distance and Received Power}{subsection.4.2}{}}
\citation{YOLOv8}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Relation of received power (blue) and the inverse of the distance between two vehicles square (in orange). The figure illustrates the relation between the two quantities across time, showing that they are highly correlated in the existence of a LoS link between the two vehicles.}}{8}{figure.7}\protected@file@percent }
\newlabel{fig:distance_vs_power}{{7}{8}{Relation of received power (blue) and the inverse of the distance between two vehicles square (in orange). The figure illustrates the relation between the two quantities across time, showing that they are highly correlated in the existence of a LoS link between the two vehicles}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Beam Distributions and GPS Positions}{8}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:beam_distributions}{{\mbox  {IV-C}}{8}{Beam Distributions and GPS Positions}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Machine Vision and Image Detection}{8}{subsection.4.4}\protected@file@percent }
\newlabel{subsec:img_detection}{{\mbox  {IV-D}}{8}{Machine Vision and Image Detection}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Enabled Applications}{8}{section.5}\protected@file@percent }
\newlabel{sec:enabled_applications}{{V}{8}{Enabled Applications}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Wireless Communication Applications}{8}{subsection.5.1}\protected@file@percent }
\newlabel{subsec:wireless_application}{{\mbox  {V-A}}{8}{Wireless Communication Applications}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Optimal beam across time and corresponding beam distribution for all Scenarios show a tendency of vehicles driving in front or behind each other. Different colors represent beam indices on different phased arrays to provide panel-switching context information. Interruptions are due to sequence changing (see Section \ref {subsec:Processing}) or blockage due to other vehicles. In low SNR regimes, e.g., near sample 20000 of Scenario 36, the optimal beam becomes ambiguous.}}{9}{figure.8}\protected@file@percent }
\newlabel{fig:beam_distro}{{8}{9}{Optimal beam across time and corresponding beam distribution for all Scenarios show a tendency of vehicles driving in front or behind each other. Different colors represent beam indices on different phased arrays to provide panel-switching context information. Interruptions are due to sequence changing (see Section \ref {subsec:Processing}) or blockage due to other vehicles. In low SNR regimes, e.g., near sample 20000 of Scenario 36, the optimal beam becomes ambiguous}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Beam density across 30º-bin angular space for all Scenarios.}}{9}{figure.9}\protected@file@percent }
\newlabel{fig:beam_density}{{9}{9}{Beam density across 30º-bin angular space for all Scenarios}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Relative orientation across 30º-bin angular space for all Scenarios.}}{9}{figure.10}\protected@file@percent }
\newlabel{fig:pos_density}{{10}{9}{Relative orientation across 30º-bin angular space for all Scenarios}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Example output from running YOLOv8 in image detection mode in a 180º front view image, belonging to sample 4035 of Scenario 36. The detection result is 5 people, 3 traffic lights, 2 cars (excluding ours), and a bus.}}{10}{figure.11}\protected@file@percent }
\newlabel{fig:yolo_example}{{11}{10}{Example output from running YOLOv8 in image detection mode in a 180º front view image, belonging to sample 4035 of Scenario 36. The detection result is 5 people, 3 traffic lights, 2 cars (excluding ours), and a bus}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Results from running YOLOv8 in image detection mode in 250 thousand 180º images across all scenarios. On the left, a circular chart shows the classification percentage of the major categories. The table on the right presents finer detail in classification categories with the number of detections.}}{10}{figure.12}\protected@file@percent }
\newlabel{fig:yolo_results}{{12}{10}{Results from running YOLOv8 in image detection mode in 250 thousand 180º images across all scenarios. On the left, a circular chart shows the classification percentage of the major categories. The table on the right presents finer detail in classification categories with the number of detections}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Localization}{10}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}Sensing Applications}{11}{subsection.5.3}\protected@file@percent }
\newlabel{subsec:other_applications}{{\mbox  {V-C}}{11}{Sensing Applications}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Machine Learning Tasks}{11}{section.6}\protected@file@percent }
\newlabel{sec:ML_tasks}{{VI}{11}{Machine Learning Tasks}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Position-Aided V2V Beam Prediction}{11}{subsection.6.1}\protected@file@percent }
\newlabel{subsec:position-beam}{{\mbox  {VI-A}}{11}{Position-Aided V2V Beam Prediction}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}Approach}{11}{subsection.6.2}\protected@file@percent }
\newlabel{subsec:approach}{{\mbox  {VI-B}}{11}{Approach}{subsection.6.2}{}}
\citation{Huber}
\citation{RANSAC}
\citation{Theil-Sen}
\citation{XGBoost}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Correlation between the AoA estimated via GPS positions and the best beam index for Scenario 36. Vertical lines show the supposed panel separation according to the direction of the incoming signal, while colors show the ground truth optimal panel selection. When colors are outside their supposed interval, the optimal panel is not the expected panel, complicating optimal beam determination from the estimated AoA. }}{12}{figure.13}\protected@file@percent }
\newlabel{fig:correlation}{{13}{12}{Correlation between the AoA estimated via GPS positions and the best beam index for Scenario 36. Vertical lines show the supposed panel separation according to the direction of the incoming signal, while colors show the ground truth optimal panel selection. When colors are outside their supposed interval, the optimal panel is not the expected panel, complicating optimal beam determination from the estimated AoA}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Fit from different linear and non-linear predictors to the AoA from GPS and beam index data from Scenario 36.}}{12}{figure.14}\protected@file@percent }
\newlabel{fig:pred_fit}{{14}{12}{Fit from different linear and non-linear predictors to the AoA from GPS and beam index data from Scenario 36}{figure.14}{}}
\newlabel{eq:base_formula}{{1}{12}{Approach}{equation.6.1}{}}
\newlabel{eq:moving_avg}{{3}{12}{Approach}{equation.6.3}{}}
\newlabel{eq:aoa}{{4}{12}{Approach}{equation.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Regression results in top-k beam prediction accuracies from using different predictors in the estimation of AoA from GPS positions. }}{13}{figure.15}\protected@file@percent }
\newlabel{fig:ml_results}{{15}{13}{Regression results in top-k beam prediction accuracies from using different predictors in the estimation of AoA from GPS positions}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}Results}{13}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion}{13}{section.7}\protected@file@percent }
\bibstyle{IEEEtran}
\bibcite{harding2014vehicle}{1}
\bibcite{zeadally2020tutorial}{2}
\bibcite{rappaport2019wireless}{3}
\bibcite{alkhateeb2023real}{4}
\bibcite{viswanathan2020communications}{5}
\bibcite{charan2022vision}{6}
\bibcite{charan2021vision}{7}
\bibcite{morais2022position}{8}
\bibcite{charan2022multi}{9}
\bibcite{jiang2022lidar}{10}
\bibcite{demirhan2022radar}{11}
\bibcite{charan2022drone}{12}
\bibcite{wu2023proactively}{13}
\bibcite{charan2022blockage}{14}
\bibcite{demirhan2023integrated}{15}
\bibcite{caesar2020nuscenes}{16}
\bibcite{llc2019waymo}{17}
\bibcite{pham20203d}{18}
\bibcite{Brostow_CamVid_2008}{19}
\bibcite{Geiger_KITTI}{20}
\bibcite{cordts2016cityscapes}{21}
\bibcite{yu2018bdd100k}{22}
\bibcite{huang2019apolloscape}{23}
\bibcite{ma2019trafficpredict}{24}
\bibcite{patil2019h3d}{25}
\bibcite{chang2019argoverse}{26}
\bibcite{houston2020thousand}{27}
\bibcite{wang2021rethinking}{28}
\bibcite{yu2022dair}{29}
\bibcite{choi2018kaist}{30}
\bibcite{collision_warning_v2v}{31}
\bibcite{collision_avoidance_60_percent}{32}
\bibcite{p2p_video_streaming}{33}
\bibcite{DeepSense}{34}
\bibcite{YOLOv8}{35}
\bibcite{Huber}{36}
\bibcite{RANSAC}{37}
\bibcite{Theil-Sen}{38}
\bibcite{XGBoost}{39}
\@writefile{toc}{\contentsline {section}{References}{14}{section*.1}\protected@file@percent }
\gdef \@abspage@last{14}
